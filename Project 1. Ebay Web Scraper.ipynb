{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0c3cba",
   "metadata": {},
   "source": [
    "# Scraping and Saving eBay Products Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea2790",
   "metadata": {},
   "source": [
    "## Project Overview:\n",
    "\n",
    "This Jupyter Notebook illustrates a comprehensive web scraping project that is designed to extract specific data about sold graphics cards from eBay. The primary tools employed in this project include the BeautifulSoup library for web scraping and Selenium for handling dynamic content. The information extracted includes the product's title, its selling price, additional details, the product link, and the date it was sold.\n",
    "\n",
    "This is the first out of four notebooks where we'll walk through the entire data pipeline process. Starting with data collection, we'll subsequently clean, organize, transform, analyze, and visualize the data.\n",
    "\n",
    "This notebook specifically focuses on the scraping and data extraction phase of the project. A number of functions are defined and utilized for parsing the webpage HTML, extracting necessary details, and handling any inconsistencies in the data. The scraped data is then stored in two formats: as a CSV file and in an SQLite database, which provides flexibility for future data handling and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edebd4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc80db",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bf16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc7b3d",
   "metadata": {},
   "source": [
    "## Define the parse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d2487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(soup):\n",
    "    products = []\n",
    "    results = soup.find_all('li', {'class': 's-item s-item__pl-on-bottom'})\n",
    "\n",
    "    for item in results:\n",
    "        title = item.find('span', {'role': 'heading'})\n",
    "        price = item.find('span', {'class': 's-item__price'})\n",
    "        subtitle = item.find('div', {'class': 's-item__subtitle'})\n",
    "        link = item.find('a', {'class': 's-item__link'})\n",
    "        sold_date = item.find('span', {'class': 'POSITIVE'})\n",
    "\n",
    "        product = {\n",
    "            'title': title.text.strip() if title else 'N/A',\n",
    "            'price': price.text.strip() if price else 'N/A',\n",
    "            'info': subtitle.text.strip() if subtitle else 'N/A',\n",
    "            'link': link['href'] if link else 'N/A',\n",
    "            'sold_date': parse_date(sold_date.text.strip()) if sold_date else 'N/A',\n",
    "        }\n",
    "\n",
    "        products.append(product)\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e152b",
   "metadata": {},
   "source": [
    "## Define the parse_date function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d98ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    if date_str.startswith('Sold'):\n",
    "        date_str = date_str[5:].strip()\n",
    "        return datetime.strptime(date_str, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2ad67",
   "metadata": {},
   "source": [
    "##  Define the get_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b055aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, user_agent):\n",
    "    options = Options()\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Add a delay to allow the page to load completely\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a8ed4",
   "metadata": {},
   "source": [
    "## Set URL and user agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db178b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ebay.com/sch/i.html?_fsrp=1&_from=R40&_nkw=graphics+card&_sacat=0&LH_Complete=1&LH_Sold=1&_fss=1&LH_SellerWithStore=1&LH_PrefLoc=1&_udlo=50&_udhi=3500&LH_ItemCondition=1000%7C1500%7C2000%7C2020%7C2500%7C3000&Brand=ASUS%7CGIGABYTE%7CEVGA%7CPowerColor%7CNVIDIA%7CPNY%7CSAPPHIRE%7CZOTAC%7CXFX%7CMSI&Chipset%2520Manufacturer=NVIDIA%7CAMD&Memory%2520Type=GDDR5%7CGDDR5X%7CGDDR6%7CGDDR6X&Chipset%252FGPU%2520Model=AMD%2520Radeon%2520R9%2520390%7CAMD%2520Radeon%2520RX%2520470%7CAMD%2520Radeon%2520RX%2520480%7CAMD%2520Radeon%2520RX%2520460%7CAMD%2520Radeon%2520R9%2520390X%7CAMD%2520Radeon%2520RX%25205500%2520XT%7CAMD%2520Radeon%2520RX%2520560%7CAMD%2520Radeon%2520RX%2520570%7CAMD%2520Radeon%2520RX%25205700%7CAMD%2520Radeon%2520RX%25205700%2520XT%7CAMD%2520Radeon%2520RX%2520580%7CAMD%2520Radeon%2520RX%25206800%7CAMD%2520Radeon%2520RX%25206800%2520XT%7CAMD%2520Radeon%2520RX%25206900%2520XT%7CNVIDIA%2520GeForce%2520GTX%25201080%7CNVIDIA%2520GeForce%2520GTX%25201080%2520Ti%7CNVIDIA%2520GeForce%2520GTX%25201660%7CNVIDIA%2520GeForce%2520GTX%2520970%7CNVIDIA%2520GeForce%2520GTX%2520980%7CNVIDIA%2520GeForce%2520GTX%2520980%2520Ti%7CNVIDIA%2520GeForce%2520GTX%2520TITAN%7CNVIDIA%2520GeForce%2520GTX%2520TITAN%2520X%7CNVIDIA%2520GeForce%2520GTX%2520TITAN%2520Xp%7CNVIDIA%2520GeForce%2520RTX%25202060%7CNVIDIA%2520GeForce%2520RTX%25202070%7CNVIDIA%2520GeForce%2520RTX%25202070%2520Founders%2520Edition%7CNVIDIA%2520GeForce%2520RTX%25202080%7CNVIDIA%2520GeForce%2520RTX%25202080%2520Founders%2520Edition%7CNVIDIA%2520GeForce%2520RTX%25202080%2520Ti%7CNVIDIA%2520GeForce%2520RTX%25202080%2520Ti%2520Founders%2520Edition%7CNVIDIA%2520GeForce%2520RTX%25203060%7CNVIDIA%2520GeForce%2520RTX%25203060%2520Ti%7CNVIDIA%2520GeForce%2520RTX%25203070%7CNVIDIA%2520GeForce%2520RTX%25203080%7CNVIDIA%2520Quadro%25204000%7CNVIDIA%2520GeForce%2520GTX%25201060%7CNVIDIA%2520GeForce%2520GTX%25201050%2520Ti%7CNVIDIA%2520GeForce%2520GTX%25201050%7CNVIDIA%2520GeForce%2520GTX%25201070%2520Ti%7CNVIDIA%2520GeForce%2520GTX%25201070%7CNVIDIA%2520GeForce%2520GT%25201030&_ipg=240&rt=nc&Memory%2520Size=11%2520GB%7C4%2520GB%7C8%2520GB%7C12%2520GB%7C6%2520GB%7C16%2520GB%7C2%2520GB%7C10%2520GB&_dcat=27386\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0bfd1f",
   "metadata": {},
   "source": [
    "## Set display options for pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec83d49",
   "metadata": {},
   "source": [
    "## Set the maximum number of pages to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369cc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pages = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e499eaa",
   "metadata": {},
   "source": [
    "## Create an empty list to store all the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcfb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364cd5ee",
   "metadata": {},
   "source": [
    "## Scrape data from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e21fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1, max_pages+1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    current_url = url + f\"&_pgn={page}\"\n",
    "    soup = get_data(current_url, user_agent)\n",
    "    products = parse(soup)\n",
    "    all_products.extend(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddeb5db",
   "metadata": {},
   "source": [
    "## Create a DataFrame from the collected products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296dd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe965b",
   "metadata": {},
   "source": [
    "## Save DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a56768",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ebay_products.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "print(f\"CSV file '{filename}' has been downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc14cd6",
   "metadata": {},
   "source": [
    "## Save DataFrame as SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801aae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filename = \"ebay_products.db\"\n",
    "conn = sqlite3.connect(database_filename)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        title TEXT,\n",
    "        price TEXT,\n",
    "        info TEXT,\n",
    "        link TEXT,\n",
    "        sold_date TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Insert data into the table\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute('''\n",
    "        INSERT INTO products (title, price, info, link, sold_date)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (row['title'], row['price'], row['info'], row['link'], row['sold_date']))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"SQLite database file '{database_filename}' has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda1593",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcea3ad",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "This Jupyter Notebook illustrates the practical implementation of web scraping with BeautifulSoup and Selenium to extract data from eBay. The acquired data about sold graphics cards is meticulously parsed and stored in two versatile formats: a CSV file and an SQLite database. This process lays the foundation for robust data analysis and application development in subsequent stages of the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
